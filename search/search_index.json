{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#how-tart-is-different-from-anka","title":"How Tart is different from Anka?","text":"<p>Under the hood Tart is using the same technology as Anka 3.0 so there should be no real difference in performance or features supported. If there is some feature missing please don't hesitate to create a feature request.</p> <p>Instead of Anka Registry, Tart can work with any OCI-compatible container registry. This provides a much more consistent and scalable experience for distributing virtual machines.</p> <p>Tart doesn't yet have an analogue of Anka Controller for managing long living VMs but soon will be.</p>"},{"location":"faq/#vm-location-on-disk","title":"VM location on disk","text":"<p>Tart stores all it's files in <code>~/.tart/</code> directory. Local images that you can run are stored in <code>~/.tart/vms/</code>. Remote images are pulled into <code>~/.tart/cache/OCIs/</code>.</p>"},{"location":"faq/#nested-virtualization-support","title":"Nested virtualization support?","text":"<p>Tart is limited by functionality of Apple's <code>Virtualization.Framework</code>. At the moment <code>Virtualization.Framework</code> doesn't support nested virtualization.</p>"},{"location":"faq/#connecting-to-a-service-running-on-host","title":"Connecting to a service running on host","text":"<p>To connect from within a virtual machine to a service running on the host machine please first make sure that the service is binded to <code>0.0.0.0</code>.</p> <p>Then from within a virtual machine you can access the service using the router's IP address that you can get either from <code>Preferences -&gt; Network</code> or by running the following command in the Terminal:</p> <pre><code>netstat -nr | grep default | head -n 1 | awk '{print $2}'\n</code></pre> <p>Note: that accessing host is only possible with the default NAT network. If you are running your virtual machines with Softnet (via <code>tart run --net-softnet &lt;VM NAME&gt;)</code>, then the network isolation is stricter and it's not only possible to access the host.</p>"},{"location":"faq/#changing-the-default-nat-subnet","title":"Changing the default NAT subnet","text":"<p>To change the default network to <code>192.168.77.1</code>:</p> <pre><code>sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.vmnet.plist Shared_Net_Address -string 192.168.77.1\n</code></pre> <p>Note that even through a network would normally be specified as <code>192.168.77.0</code>, the vmnet framework seems to treat this as a starting address too and refuses to pick up such network-like values.</p> <p>The default subnet mask <code>255.255.255.0</code> should suffice for most use-cases, however, you can also change it to <code>255.255.0.0</code>, for example:</p> <pre><code>sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.vmnet.plist Shared_Net_Mask -string 255.255.0.0\n</code></pre>"},{"location":"faq/#changing-the-default-dhcp-lease-time","title":"Changing the default DHCP lease time","text":"<p>By default, the built-in macOS DHCP server allocates IP-addresses to the VMs for the duration of 86,400 seconds (one day), which may easily cause DHCP exhaustion if you run more than ~253 VMs per day, or in other words, more than one VM every ~6 minutes.</p> <p>This issue is worked around automatically when using Softnet, however, if you don't use or can't use it, the following command will reduce the lease time from the default 86,400 seconds (one day) to 600 seconds (10 minutes):</p> <pre><code>sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.InternetSharing.default.plist bootpd -dict DHCPLeaseTimeSecs -int 600\n</code></pre> <p>Note that this tweak persists across reboots, so normally you'll only need to do it once per new host.</p>"},{"location":"licensing/","title":"Support & Licensing","text":"<p>Both Tart Virtualization and Orchard Orchestration are licensed under Fair Source License. Usage on personal computers including personal workstations is royalty-free, but organizations that exceed a certain number of server installations (100 CPU cores for Tart and/or 4 hosts for Orchard) will be required to obtain a paid license.</p> Host CPU Core usage <p>The virtual CPU cores of Tart VMs are not tied to specific physical cores of the host CPU. Instead, for optimal performance Tart VMs will automatically try to balance compute between all available cores of the host CPU. As a result, all performance and energy-efficient cores of the host CPU are always counted towards the license usage.</p>"},{"location":"licensing/#license-tiers","title":"License Tiers","text":""},{"location":"licensing/#free-tier","title":"Free Tier","text":"<p>By default, when no license is purchased, it is assumed that an organization is using a Free Tier license. You can find the Free Tier license text in Tart and Orchard repositories.</p> <p>Free Tier license has a 100 CPU core limit for Tart and 4 Orchard Workers limit for Orchard.</p> Usage Scenarios Examples <p>Here are a few examples that fit into the free tier:</p> <ul> <li>Using Tart on 12 Mac Minis with 8 CPUs each running up to 24 VMs in parallel.</li> <li>Creating an Orchard cluster of 4 Mac Studio workers with 24 CPUs each.</li> </ul> <p>Here are a few examples that do not fit into the free tier:</p> <ul> <li>Using Tart on 13 Mac Minis with 8 CPUs each.</li> <li>Creating an Orchard cluster of 5 Mac Minis workers with 8 CPUs each.</li> </ul>"},{"location":"licensing/#gold-tier","title":"Gold Tier","text":"<p>If an organization wishes to exceed the limits of the Free Tier license, a purchase of the Gold Tier License is required, which costs $1000 per month.</p> <p>Gold Tier license has a 500 CPU core limit for Tart and 20 Orchard Workers limit for Orchard.</p>"},{"location":"licensing/#platinum-tier","title":"Platinum Tier","text":"<p>If an organization wishes to exceed the limits of the Gold Tier license, a purchase of the Platinum Tier License is required, which costs $5000 per month.</p> <p>Platinum Tier license has a 5,000 CPU core limit for Tart and 200 Orchard Workers limit for Orchard.</p>"},{"location":"licensing/#diamond-tier","title":"Diamond Tier","text":"<p>For organizations that wish to exceed the limits of the Platinum Tier license, a purchase of a custom Diamond Tier License is required, which costs $1 per CPU core per month and gives the ability to run unlimited Orchard Workers.</p>"},{"location":"licensing/#get-the-license","title":"Get the license","text":"<p>If your organization is interested in purchasing one of the license tiers, please email licensing@cirruslabs.org.</p> <p>You can see a template of a license subscription agreement here.</p> <p>Running on AWS?</p> <p>There are official AMIs for EC2 Mac Instances with preconfigured Tart installation that is optimized to work within AWS infrastructure.</p>"},{"location":"licensing/#general-support","title":"General Support","text":"<p>The best way to ask general questions about particular use cases is to email our support team at support@cirruslabs.org. Our support team is trying our best to respond ASAP, but there is no guarantee on a response time unless your organization has a paid license subscription which includes Priority Support.</p> <p>If you have a feature request or noticed lack of some documentation please feel free to create a GitHub issue. Our support team will answer it by replying to the issue or by updating the documentation.</p>"},{"location":"licensing/#priority-support","title":"Priority Support","text":"<p>In addition to the general support we provide a Priority Support with guaranteed response times included in all the paid license tiers.</p> Severity Support Impact First Response Time SLA Hours How to Submit 1 Emergency (service is unavailable or completely unusable). 30 minutes 24x7 Please use urgent email address. 2 Highly Degraded (Important features unavailable or extremely slow; No acceptable workaround). 4 hours 24x5 Please use priority email address. 3 Medium Impact. 8 hours 24x5 Please use priority email address. 4 Low Impact. 24 hours 24x5 Please use regular support email address. Make sure to send the email from your corporate email. <p><code>24x5</code> means period of time from 9AM on Monday till 5PM on Friday in EST timezone.</p> Support Impact Definitions <ul> <li>Severity 1 - Your installation of Orchard is unavailable or completely unusable. An urgent issue can be filed and   our On-Call Support Engineer will respond within 30 minutes. Example: Orchard Controller is showing 502 errors for all users.</li> <li>Severity 2 - Orchard installation is Highly Degraded. Significant Business Impact. Important features are unavailable   or extremely slowed, with no acceptable workaround.</li> <li>Severity 3 - Something is preventing normal service operation. Some Business Impact. Important features of Tart or Orchard   are unavailable or somewhat slowed, but a workaround is available.</li> <li>Severity 4 - Questions or Clarifications around features or documentation. Minimal or no Business Impact.   Information, an enhancement, or documentation clarification is requested, but there is no impact on the operation of Tart and/or Orchard.</li> </ul> <p>How to submit a priority or an urgent issue</p> <p>Once your organization obtains a license, members of your organization will get access to separate support emails specified in your subscription contract.</p>"},{"location":"quick-start/","title":"Quick Start","text":"<p>Try running a Tart VM on your Apple Silicon device running macOS 13.0 (Ventura) or later (will download a 25 GB image):</p> <pre><code>brew install cirruslabs/cli/tart\ntart clone ghcr.io/cirruslabs/macos-sonoma-base:latest sonoma-base\ntart run sonoma-base\n</code></pre> Manual installation from a release archive <p>It's also possible to manually install <code>tart</code> binary from the latest released archive:</p> <pre><code>curl -LO https://github.com/cirruslabs/tart/releases/latest/download/tart.tar.gz\ntar -xzvf tart.tar.gz\n./tart.app/Contents/MacOS/tart clone ghcr.io/cirruslabs/macos-sonoma-base:latest sonoma-base\n./tart.app/Contents/MacOS/tart run sonoma-base\n</code></pre> <p>Please note that <code>./tart.app/Contents/MacOS/tart</code> binary is required to be used in order to trick macOS to pick <code>tart.app/Contents/embedded.provisionprofile</code> for elevated privileges that Tart needs.</p> <p> </p>"},{"location":"quick-start/#ssh-access","title":"SSH access","text":"<p>If the guest VM is running and configured to accept incoming SSH connections you can conveniently connect to it like so:</p> <pre><code>ssh admin@$(tart ip sonoma-base)\n</code></pre> <p>Running scripts inside Tart virtual machines</p> <p>We recommend using Cirrus CLI to run scripts and/or retrieve artifacts from within Tart virtual machines. Alternatively, you can use plain ssh connection and <code>tart ip</code> command:</p> <pre><code>brew install sshpass\nsshpass -p admin ssh -o \"StrictHostKeyChecking no\" admin@$(tart ip sonoma-base) \"uname -a\"\nsshpass -p admin ssh -o \"StrictHostKeyChecking no\" admin@$(tart ip sonoma-base) &lt; script.sh\n</code></pre>"},{"location":"quick-start/#mounting-directories","title":"Mounting directories","text":"<p>To mount a directory, run the VM with the <code>--dir</code> argument:</p> <pre><code>tart run --dir=project:~/src/project vm\n</code></pre> <p>Here, the <code>project</code> specifies a mount name, whereas the <code>~/src/project</code> is a path to the host's directory to expose to the VM.</p> <p>It is also possible to mount directories in read-only mode by adding a third parameter, <code>ro</code>:</p> <pre><code>tart run --dir=project:~/src/project:ro vm\n</code></pre> <p>To mount multiple directories, repeat the <code>--dir</code> argument for each directory:</p> <pre><code>tart run --dir=www1:~/project1/www --dir=www2:~/project2/www\n</code></pre> <p>Note that the first parameter in each <code>--dir</code> argument must be unique, otherwise only the last <code>--dir</code> argument using that name will be used.</p> <p>Note: to use the directory mounting feature, the host needs to run macOS 13.0 (Ventura) or newer.</p>"},{"location":"quick-start/#accessing-mounted-directories-in-macos-guests","title":"Accessing mounted directories in macOS guests","text":"<p>All shared directories are automatically mounted to <code>/Volumes/My Shared Files</code> directory.</p> <p>The directory we've mounted above will be accessible from the <code>/Volumes/My Shared Files/project</code> path inside a guest VM.</p> <p>Note: to use the directory mounting feature, the guest VM needs to run macOS 13.0 (Ventura) or newer.</p> Changing mount location <p>It is possible to remount the directories after a virtual machine is started by running the following commands:</p> <pre><code>sudo umount \"/Volumes/My Shared Files\"\nmkdir ~/workspace\nmount_virtiofs com.apple.virtio-fs.automount ~/workspace\n</code></pre> <p>After running the above commands the direcory will be available at <code>~/workspace/project</code></p>"},{"location":"quick-start/#accessing-mounted-directories-in-linux-guests","title":"Accessing mounted directories in Linux guests","text":"<p>To be able to access the shared directories from the Linux guest, you need to manually mount the virtual filesystem first:</p> <pre><code>mount -t virtiofs com.apple.virtio-fs.automount /mnt/shared\n</code></pre> <p>The directory we've mounted above will be accessible from the <code>/mnt/shared/project</code> path inside a guest VM.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"integrations/cirrus-cli/","title":"Cirrus CLI","text":"<p>Tart itself is only responsible for managing virtual machines, but we've built Tart support into a tool called Cirrus CLI also developed by Cirrus Labs. Cirrus CLI is a command line tool with one configuration format to execute common CI steps (run a script, cache a folder, etc.) locally or in any CI system. We built Cirrus CLI to solve \"But it works on my machine!\" problem.</p> <p>Here is an example of a <code>.cirrus.yml</code> configuration file which will start a Tart VM, will copy over working directory and will run scripts and other instructions inside the virtual machine:</p> <pre><code>task:\nname: hello\nmacos_instance:\n# can be a remote or a local virtual machine\nimage: ghcr.io/cirruslabs/macos-sonoma-base:latest\nhello_script:\n- echo \"Hello from within a Tart VM!\"\n- echo \"Here is my CPU info:\"\n- sysctl -n machdep.cpu.brand_string\n- sleep 15\n</code></pre> <p>Put the above <code>.cirrus.yml</code> file in the root of your repository and run it with the following command:</p> <pre><code>brew install cirruslabs/cli/cirrus\ncirrus run\n</code></pre> <p></p> <p>Cirrus CI already leverages Tart to power its macOS cloud infrastructure. The <code>.cirrus.yml</code> config from above will just work in Cirrus CI and your tasks will be executed inside Tart VMs in our cloud.</p> <p>Note: Cirrus CI only allows images managed and regularly updated by us.</p>"},{"location":"integrations/cirrus-cli/#retrieving-artifacts-from-within-tart-vms","title":"Retrieving artifacts from within Tart VMs","text":"<p>In many cases there is a need to retrieve particular files or a folder from within a Tart virtual machine. For example, the below <code>.cirrus.yml</code> configuration defines a single task that builds a <code>tart</code> binary and exposes it via <code>artifacts</code> instruction:</p> <pre><code>task:\nname: Build\nmacos_instance:\nimage: ghcr.io/cirruslabs/macos-sonoma-xcode:latest\nbuild_script: swift build --product tart\nbinary_artifacts:\npath: .build/debug/tart\n</code></pre> <p>Running Cirrus CLI with <code>--artifacts-dir</code> will write defined <code>artifacts</code> to the provided local directory on the host:</p> <pre><code>cirrus run --artifacts-dir artifacts\n</code></pre> <p>Note that all retrieved artifacts will be prefixed with the associated task name and <code>artifacts</code> instruction name. For the example above, <code>tart</code> binary will be saved to <code>$PWD/artifacts/Build/binary/.build/debug/tart</code>.</p>"},{"location":"integrations/github-actions/","title":"Cirrus Runners for GitHub Actions","text":"<p>Cirrus Runners is the fastest and most cost-efficient way to get your current CI workflows to benefit from Apple Silicon hardware. No need to manage infrastructure or migrate to another CI provider. Your actions will be executed in clean macOS virtual machines with 4 Apple M2 cores.</p>"},{"location":"integrations/github-actions/#testimonials-from-customers","title":"Testimonials from customers","text":"<p>Max Lapides, Senior Mobile Engineer at Tonal:</p> <p>Previously, we were using the GitHub\u2011hosted macOS runners and our iOS build took ~30 minutes. Now with Cirrus Runners, the iOS build only takes ~12 minutes. That\u2019s a huge boost to our productivity, and for only $150/month per runner it is much less expensive too.</p> <p>John A., Software Engineer at GitKraken:</p> <p>GitHub Actions MacOS-x86 runners have become increasingly unreliable, so we're moving our Mac builds over to arm64 because Cirrus Labs' M1 runners are not only ~3 times faster, they've also been far more stable.</p> <p>Sebastian Jachec, Mobile Engineer at Daybridge:</p> <p>It\u2019s been plain-sailing with the Cirrus Runners \u2014 they\u2019ve been great! They\u2019re consistently 60+% faster on workflows that we previously used Github Actions\u2019 macOS runners for.</p>"},{"location":"integrations/github-actions/#pricing","title":"Pricing","text":"<p>Each Cirrus Runner costs $150 a month and there is no limit on the amount of minutes for your actions. We recommend to purchase several Cirrus Runners depending on your team size, so you can run actions in parallel. Note that you can change your subscription at any time via this page or by emailing support@cirruslabs.org.</p>"},{"location":"integrations/github-actions/#priority-support","title":"Priority Support","text":"<p>Subscriptions of 20 or more Cirrus Runners include access to Priority Support. Please contact sales@cirruslabs.org in order to get all the details.</p>"},{"location":"integrations/github-actions/#cpu-and-memory-resources-of-cirrus-runners","title":"CPU and Memory resources of Cirrus Runners","text":"<p>By default, a single Cirrus Runner is allocated with 4 M2 cores and 12 GB of unified memory which is enough for most of the workloads. For workloads that require more resources it is possible to use XL Cirrus Runners which have twice the resources: a full M2 chip with 8 cores and 24 GB of unified memory. Note that a single XL Cirrus Runner also uses twice the concurrency.</p> <p>In order to use an XL Cirrus Runner for a job please append <code>-xl</code> suffix to your <code>runs-on</code> property. More on that down below.</p>"},{"location":"integrations/github-actions/#installation","title":"Installation","text":"<p>Once you configure Cirrus Runners App for your organization, you'll be redirected to a checkout page powered by Stripe. During the checkout process you'll be able to configure a subscription for a desired amount of parallel Cirrus Runners and try it for free for 10 days.</p> <p>Once configured, please follow instruction below. If you have any questions please contact support@cirruslabs.org. Subscriptions with more than 10 runners also include Priority Support </p>"},{"location":"integrations/github-actions/#configuring-cirrus-runners","title":"Configuring Cirrus Runners","text":"<p>In order for Cirrus Runners to be used by your GitHub Actions workflow jobs, specify a desired image in the <code>runs-on</code> property.</p> Default Cirrus RunnerXL Cirrus Runner <pre><code>name: Tests\njobs:\ntest:\nruns-on: ghcr.io/cirruslabs/macos-sonoma-xcode:latest\n</code></pre> <pre><code>name: Integration Tests\njobs:\ntest:\nruns-on: ghcr.io/cirruslabs/macos-sonoma-xcode:latest-xl\n</code></pre> <p>List of all available images can be found in this repository.</p> <p>Note that Tart VM images don't have the same set of pre-installed packages as the official Intel GitHub runners. If something is missing please create an issue within this repository.</p> <p>When workflows are executing you'll see Cirrus on-demand runners on your organization's settings page at <code>https://github.com/organizations/&lt;ORGANIZATION&gt;/settings/actions/runners</code>. Note that Cirrus Runners will get added to the default runner group.</p> <p>Using Cirrus Runners with public repositories</p> <p>By default, only private repositories can access runners in a default runner group, but you can override this in your organization's settings:</p> <p><code>https://github.com/organizations/&lt;YOUR ORGANIZATION NAME&gt;/settings/actions/runner-groups/1</code></p> <p></p>"},{"location":"integrations/github-actions/#dashboard","title":"Dashboard","text":"<p>You can also see the status of your runners on the Cirrus Runners Dashboard. This dashboard also provides insights into price performance of your Cirrus Runners. Please check out this blog post to learn more about what this dashboard can do for you.</p> <p></p>"},{"location":"integrations/github-actions/#data-handling-flow","title":"Data handling flow","text":"<p>By design Cirrus Runners service never sees any of your secrets or source code and acts as compute platform with the lastest Apple Silicon hardware that can quickly allocate CPU/Memory resources for your jobs.</p> <p>Here is a high-level overview of how Cirrus Runners service manages runners for your organization:</p> <ul> <li>Cirrus Runner GitHub App is subscribed to <code>workflow_job</code>.</li> <li> <p>Upon receiving a new event targeting Cirrus Runners via <code>runs-on</code> property the following steps take place:</p> <ul> <li>Non-personal information about your job is saved to perform health checking of Cirrus Runners execution.</li> <li>Cirrus Runners GitHub App has only one permission that allows generating temporary registration tokens for   self-hosted GitHub Actions Runners. Note that Cirrus Runners GitHub App itself doesn't have access to contents of   repositories in your organization.</li> <li>Cirrus Runners Service creates a new single use Tart VM, generates a temporary registration tokens for self-hosted runners   and passes it without storing inside the VM for the GitHub Actions Runner service to start a ephemeral runner.</li> </ul> </li> <li> <p>Cirrus Runners service continuously monitors health of the Tart VM executing your job to make sure it runs to completion.</p> </li> <li>After the job finishes the ephemeral Tart VM is getting destroyed with all the information of the job run.</li> </ul> <p>If you have any questions or concerns please feel free to reach out to support@cirruslabs.org.</p>"},{"location":"integrations/gitlab-runner/","title":"GitLab Runner Executor","text":"<p>It is possible to run GitLab jobs in isolated ephemeral Tart Virtual Machines via Tart Executor. Tart Executor utilizes custom executor feature of GitLab Runner.</p>"},{"location":"integrations/gitlab-runner/#basic-configuration","title":"Basic Configuration","text":"<p>Configuring Tart Executor for GitLab Runner is as simple as installing <code>gitlab-tart-executor</code> binary from Homebrew:</p> <pre><code>brew install cirruslabs/cli/gitlab-tart-executor\n</code></pre> <p>And updating configuration of your self-hosted GitLab Runner to use <code>gitlab-tart-executor</code> binary:</p> <pre><code>concurrent = 2\n[[runners]]\n# ...\nexecutor = \"custom\"\nbuilds_dir = \"/Users/admin/builds\" # directory inside the \ncache_dir = \"/Users/admin/cache\"\n[runners.feature_flags]\nFF_RESOLVE_FULL_TLS_CHAIN = false\n[runners.custom]\nprepare_exec = \"gitlab-tart-executor\"\nprepare_args = [\"prepare\"]\nrun_exec = \"gitlab-tart-executor\"\nrun_args = [\"run\"]\ncleanup_exec = \"gitlab-tart-executor\"\ncleanup_args = [\"cleanup\"]\n</code></pre> <p>Now you can use Tart Images in your <code>.gitlab-ci.yml</code>:</p> <pre><code># You can use any remote Tart Image.\n# Tart Executor will pull it from the registry and use it for creating ephemeral VMs.\nimage: ghcr.io/cirruslabs/macos-sonoma-base:latest\ntest:\ntags:\n- tart-installed # in case you tagged runners with Tart Executor installed\nscript:\n- uname -a\n</code></pre> <p>For more advanced configuration please refer to GitLab Tart Executor repository.</p>"},{"location":"integrations/vm-management/","title":"Managing Virtual Machine","text":""},{"location":"integrations/vm-management/#creating-from-scratch","title":"Creating from scratch","text":"<p>Tart supports macOS and Linux virtual machines. All commands like <code>run</code> and <code>pull</code> work the same way regarding of the underlying OS a particular VM image has. The only difference is how such VM images are created. Please check sections below for macOS and Linux instructions.</p>"},{"location":"integrations/vm-management/#creating-a-macos-vm-image-from-scratch","title":"Creating a macOS VM image from scratch","text":"<p>Tart can create VMs from <code>*.ipsw</code> files. You can download a specific <code>*.ipsw</code> file here or you can use <code>latest</code> instead of a path to <code>*.ipsw</code> to download the latest available version:</p> <pre><code>tart create --from-ipsw=latest sonoma-vanilla\ntart run sonoma-vanilla\n</code></pre> <p>After the initial booting of the VM you'll need to manually go through the macOS installation process. As a convention we recommend creating an <code>admin</code> user with an <code>admin</code> password. After the regular installation please do some additional modifications in the VM:</p> <ol> <li>Enable Auto-Login. Users &amp; Groups -&gt; Login Options -&gt; Automatic login -&gt; admin.</li> <li>Allow SSH. Sharing -&gt; Remote Login</li> <li>Disable Lock Screen. Preferences -&gt; Lock Screen -&gt; disable \"Require Password\" after 5.</li> <li>Disable Screen Saver.</li> <li>Run <code>sudo visudo</code> in Terminal, find <code>%admin ALL=(ALL) ALL</code> add <code>admin ALL=(ALL) NOPASSWD: ALL</code> to allow sudo without a password.</li> </ol>"},{"location":"integrations/vm-management/#creating-a-linux-vm-image-from-scratch","title":"Creating a Linux VM image from scratch","text":"<p>Linux VMs are supported on hosts running macOS 13.0 (Ventura) or newer.</p> <pre><code># Create a bare VM\ntart create --linux ubuntu\n\n# Install Ubuntu\ntart run --disk focal-desktop-arm64.iso ubuntu\n\n# Run VM\ntart run ubuntu\n</code></pre> <p>After the initial setup please make sure your VM can be SSH-ed into by running the following commands inside your VM:</p> <pre><code>sudo apt update\nsudo apt install -y openssh-server\nsudo ufw allow ssh\n</code></pre>"},{"location":"integrations/vm-management/#configuring-a-vm","title":"Configuring a VM","text":"<p>By default, a tart VM uses 2 CPUs and 4 GB of memory with a <code>1024x768</code> display. This can be changed with <code>tart set</code> command. Please refer to <code>tart set --help</code> for additional details.</p>"},{"location":"integrations/vm-management/#building-with-packer","title":"Building with Packer","text":"<p>Please refer to Tart Packer Plugin repository for setup instructions. Here is an example of a template to build a local image based of a remote image:</p> <pre><code>packer {\nrequired_plugins {\ntart = {\nversion = \"&gt;= 0.5.3\"\nsource  = \"github.com/cirruslabs/tart\"\n}\n}\n}\nsource \"tart-cli\" \"tart\" {\nvm_base_name = \"ghcr.io/cirruslabs/macos-sonoma-base:latest\"\nvm_name      = \"my-custom-sonoma\"\ncpu_count    = 4\nmemory_gb    = 8\ndisk_size_gb = 70\nssh_password = \"admin\"\nssh_timeout  = \"120s\"\nssh_username = \"admin\"\n}\nbuild {\nsources = [\"source.tart-cli.tart\"]\nprovisioner \"shell\" {\ninline = [\"echo 'Disabling spotlight indexing...'\", \"sudo mdutil -a -i off\"]\n}\n  # more provisioners\n}\n</code></pre> <p>Here is a repository with Packer templates used to build all the images managed by us.</p>"},{"location":"integrations/vm-management/#working-with-a-remote-oci-container-registry","title":"Working with a Remote OCI Container Registry","text":"<p>For example, let's say you want to push/pull images to a registry hosted at https://acme.io/.</p>"},{"location":"integrations/vm-management/#registry-authorization","title":"Registry Authorization","text":"<p>First, you need to log in and save credential for <code>acme.io</code> host via <code>tart login</code> command:</p> <pre><code>tart login acme.io\n</code></pre> <p>Credentials are securely stored in Keychain.</p> <p>In addition, Tart supports Docker credential helpers if defined in <code>~/.docker/config.json</code>.</p> <p>Finally, <code>TART_REGISTRY_USERNAME</code> and <code>TART_REGISTRY_PASSWORD</code> environment variables allow to override authorization for all registries which might useful for integrating with your CI's secret management.</p>"},{"location":"integrations/vm-management/#pushing-a-local-image","title":"Pushing a Local Image","text":"<p>Once credentials are saved for <code>acme.io</code>, run the following command to push a local images remotely with two tags:</p> <pre><code>tart push my-local-vm-name acme.io/remoteorg/name:latest acme.io/remoteorg/name:v1.0.0\n</code></pre>"},{"location":"integrations/vm-management/#pulling-a-remote-image","title":"Pulling a Remote Image","text":"<p>You can either pull an image:</p> <pre><code>tart pull acme.io/remoteorg/name:latest\n</code></pre> <p>...or instantiate a VM from a remote image:</p> <pre><code>tart clone acme.io/remoteorg/name:latest my-local-vm-name\n</code></pre> <p>This invocation calls the <code>tart pull</code> implicitly (if the image is not being present) before doing the actual cloning.</p>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/announcement/","title":"announcement","text":""},{"location":"blog/category/orchard/","title":"orchard","text":""}]}